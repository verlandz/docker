# 5. Docker Swarm
Docker swarm is a orchestration tool to maintain multiple machine (physical/virtual) who are running docker engines into a cluster.\
There's 2 role:
- `Manager`: node who do command, start/stop/control docker nodes, and making sure all running well. Manager also worker.
- `Worker`: node who only work, can't do command.

Features:
- Load Balancing: balancing running services to active node.
- Decentralized Access: we can access manager/worker from anywhere. 
- Rolling Updates: update all services in one command.
- [and many more](https://docs.docker.com/engine/swarm/#feature-highlights).


Before we start, keep in mind that all orchestra cmd only available in manager node, such as:
```
docker service ...
docker node ...
```

## 5.1. Create virtual machine with docker-machine
- Before we start `docker swarm`, let's create virtual machine. The purpose is to simulate of having different machine (assuming it physical located in different places).
- Please create machine for `manager1`, `worker1` and `worker2` too. Here's the example of `manager1`. It might take some time, so better to run in paralel on different terminal.
```
root@verlandz:~# docker-machine create --driver virtualbox manager1
Running pre-create checks...
Creating machine...
(manager1) Copying /root/.docker/machine/cache/boot2docker.iso to /root/.docker/machine/machines/manager1/boot2docker.iso...
(manager1) Creating VirtualBox VM...
(manager1) Creating SSH key...
(manager1) Starting the VM...
(manager1) Check network to re-create if needed...
(manager1) Waiting for an IP...
Waiting for machine to be running, this may take a few minutes...
Detecting operating system of created instance...
Waiting for SSH to be available...
Detecting the provisioner...
Provisioning with boot2docker...
Copying certs to the local machine directory...
Copying certs to the remote machine...
Setting Docker configuration on the remote daemon...

This machine has been allocated an IP address, but Docker Machine could not
reach it successfully.

SSH for the machine should still work, but connecting to exposed ports, such as
the Docker daemon port (usually <ip>:2376), may not work properly.

You may need to add the route manually, or use another related workaround.

This could be due to a VPN, proxy, or host file configuration issue.

You also might want to clear any VirtualBox host only interfaces you are not using.
Checking connection to Docker...
Docker is up and running!
To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env manager1
```
- Let's see the list of machine that we created.
```
root@verlandz:~# docker-machine ls
NAME       ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
manager1   -        virtualbox   Running   tcp://192.168.99.100:2376           v19.03.12   
worker1    -        virtualbox   Running   tcp://192.168.99.101:2376           v19.03.12   
worker2    -        virtualbox   Running   tcp://192.168.99.102:2376           v19.03.12
```
- Let's open 3 terminal for each machine, and do ssh there\
terminal 1: `docker-machine ssh manager1`\
terminal 2: `docker-machine ssh worker1`\
terminal 3: `docker-machine ssh worker2`

## 5.2. Init docker swarm
- Keep in mind that currently `manager1`, `worker1` and `worker2` are equal. Now let's init the swarn and assign machine `manager1` as `manager`.
```
root@verlandz:~# docker-machine ip manager1
192.168.99.100

root@verlandz:~# docker-machine ssh manager1
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@manager1:~$ docker swarm init --advertise-addr 192.168.99.100                                                                                                                        
Swarm initialized: current node (ill4862sqfashbjr2njq967f6) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-62yhu2eli7pf41fsd0p1as3z473k6kde9xof7svvdpnqh2f1g3-c9umho35hzosgtenc1z4619fq 192.168.99.100:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions
```

- Copy the token cmd and run it inside `worker1` and `worker2`
```
docker@worker1:~$ docker swarm join --token SWMTKN-1-62yhu2eli7pf41fsd0p1as3z473k6kde9xof7svvdpnqh2f1g3-c9umho35hzosgtenc1z4619fq 192.168.99.100:2377
This node joined a swarm as a worker.
```
```
docker@worker2:~$ docker swarm join --token SWMTKN-1-62yhu2eli7pf41fsd0p1as3z473k6kde9xof7svvdpnqh2f1g3-c9umho35hzosgtenc1z4619fq 192.168.99.100:2377
This node joined a swarm as a worker
```

- If you forgot the token, you can try this.
```
docker@manager1:~$ docker swarm join-token manager                                                                                                                                          
To add a manager to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-62yhu2eli7pf41fsd0p1as3z473k6kde9xof7svvdpnqh2f1g3-6sq1qlr5hluuntdvna64tfrum 192.168.99.100:2377

docker@manager1:~$ docker swarm join-token worker                                                                                                                                           
To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-62yhu2eli7pf41fsd0p1as3z473k6kde9xof7svvdpnqh2f1g3-c9umho35hzosgtenc1z4619fq 192.168.99.100:2377
```
Keep in mind, that you can create many worker and manager, but for manager only 1 who play the role, while the others will act as substitute if manager is down.

- Since `manager1` already become `manager`, try to check which node who part of this swarm and current running services.
```
docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Active              Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active                                  19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Ready               Active                                  19.03.12

docker@manager1:~$ docker service ls                                                                                                                                                        
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
```

## 5.3. Run service inside docker swarm
There's two mode to run the service which is `global` and `replica`.
- `global`: **always** assign 1 service for each node, can't be more or less. Even new node who just join the swarm will auto assign with the service. You need to add `--mode global` during `docker create`
- `replica`: one node can run up to N services (it can't be empty). You need to add `--replicas <NUMBER_REPLICA>` during `docker create` or it will behave as `--replicas 1`.

From this, I will choose `replica` because I want to demo the scaling feature and proving that node can access the app without running the app but in condition one cluster with node who run the app.\
Let's create the service with nginx

```
docker@manager1:~$ docker service create --name my_nginx -p 80:80 --replicas 2 nginx:1.18                                                                                                   
yn6zjy77naix787rj5z72e0rd
overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged 

docker@manager1:~$ docker service ls                                                                                                                                                        
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
yn6zjy77naix        my_nginx            replicated          2/2                 nginx:1.18          *:80->80/tcp

docker@manager1:~$ docker service ps my_nginx                                                                                                                                               
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS
r6ydx6jedyx6        my_nginx.1          nginx:1.18          worker1             Running             Running 20 seconds ago                       
wv69t569wdtr        my_nginx.2          nginx:1.18          manager1            Running             Running 16 seconds ago 
```
`docker service ls`: to know what services that run in your swarm.\
`docker service ps my_nginx `: to know what node that running service my_nginx.\

You may wondering why I only create 2 replica although we have 3 active node. Because I want to show you, if the app is down/missing in some node, you still can access in that node as long there's port open in that cluster. ***That's because the port number internally exported to all the nodes inside the cluster.***
Now try to:
```
docker@manager1:~$ curl localhost:80
docker@worker1:~$ curl localhost:80
docker@worker2:~$ curl localhost:80
```

All should be return this, including the node who don't run the nginx (worker2)
```
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

## 5.4. Rolling updates
Let's update the images from `nginx:1.18` to `nginx:1.19` with this cmd
```
docker@manager1:~$ docker service update --image nginx:1.19 my_nginx                                                                                                                        
my_nginx
overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged 

docker@manager1:~$ docker service ps my_nginx                                                                                                                                               
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE                 ERROR               PORTS
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 53 seconds ago                            
r6ydx6jedyx6         \_ my_nginx.1      nginx:1.18          worker1             Shutdown            Shutdown about a minute ago                       
p9fl6rakhw02        my_nginx.2          nginx:1.19          manager1            Running             Running 39 seconds ago                            
wv69t569wdtr         \_ my_nginx.2      nginx:1.18          manager1            Shutdown            Shutdown 50 seconds ago                           
```
As you can see, the old images are `shutdown` while the new one are `running`


# 5.5. Scaling
We can also scale up/down the number of services and it will automatically balance for each node
```
docker@manager1:~$ docker service scale my_nginx=6                                                                                                                                          
my_nginx scaled to 6
overall progress: 6 out of 6 tasks 
1/6: running   [==================================================>] 
2/6: running   [==================================================>] 
3/6: running   [==================================================>] 
4/6: running   [==================================================>] 
5/6: running   [==================================================>] 
6/6: running   [==================================================>] 
verify: Service converged

docker@manager1:~$ docker service ls                                                                                                                                                        
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
yn6zjy77naix        my_nginx            replicated          6/6                 nginx:1.19          *:80->80/tcp

docker@manager1:~$ docker service ps my_nginx | grep Running                                                                                                                                
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 6 minutes ago                            
p9fl6rakhw02        my_nginx.2          nginx:1.19          manager1            Running             Running 6 minutes ago                            
h3xvzdtos0vw        my_nginx.3          nginx:1.19          worker2             Running             Running 37 seconds ago                           
n6nun0crz3ai        my_nginx.4          nginx:1.19          worker2             Running             Running 37 seconds ago                           
sb3bxf2pzx7f        my_nginx.5          nginx:1.19          worker1             Running             Running about a minute ago                       
poc3670d9k9c        my_nginx.6          nginx:1.19          manager1            Running             Running about a minute ago                       
```
As you can see, the `REPLICAS` become `6/6` and each node got 2 services which indicate the load balance is working.

# 5.6. Status, Availability & Manager Status
References: https://docs.docker.com/engine/swarm/manage-nodes/#list-nodes

- Let's test `drain` on `worker2`.
```
docker@manager1:~$ docker node update --availability drain worker2                                                                                                                          
worker2

docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Active              Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active                                  19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Ready               Drain                                   19.03.12

docker@manager1:~$ docker service ps my_nginx  | grep Running                                                                                                                               
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 4 minutes ago                                   
hxbqfiqydow3        my_nginx.2          nginx:1.19          manager1            Running             Running 3 minutes ago                                   
9d0umdwmrkvu        my_nginx.3          nginx:1.19          manager1            Running             Running 37 seconds ago                                  
mvzbbce3305a        my_nginx.4          nginx:1.19          worker1             Running             Running 37 seconds ago                                  
sb3bxf2pzx7f        my_nginx.5          nginx:1.19          worker1             Running             Running 4 minutes ago                                   
6u8a459bflc3        my_nginx.6          nginx:1.19          manager1            Running             Running 3 minutes ago
```
Base on ref, the `drain` will stop scheduler to assign task to it, and will shedules them to another active node. You can see now `manager1` and `worker1` are having 3 services each.

- Let's set back to `active` for `worker2`.
```
docker@manager1:~$ docker node update --availability active worker2                                                                                                                         
worker2

docker@manager1:~$ docker service ps my_nginx  | grep Running                                                                                                                               
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 8 minutes ago                                   
hxbqfiqydow3        my_nginx.2          nginx:1.19          manager1            Running             Running 7 minutes ago                                   
9d0umdwmrkvu        my_nginx.3          nginx:1.19          manager1            Running             Running 4 minutes ago                                   
mvzbbce3305a        my_nginx.4          nginx:1.19          worker1             Running             Running 4 minutes ago                                   
sb3bxf2pzx7f        my_nginx.5          nginx:1.19          worker1             Running             Running 8 minutes ago                                   
6u8a459bflc3        my_nginx.6          nginx:1.19          manager1            Running             Running 7 minutes ago 
```
Surprisingly, the services not back to balance after `worker2` back from `drain` to `active`.

- Let's kill `worker2` machine.
```
root@verlandz:~# docker-machine stop worker2
Stopping "worker2"...
Machine "worker2" was stopped.
```
```
docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Active              Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active                                  19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Down                Active                                  19.03.12
```

If the machine is down it will change the status to `Down`. But the `AVAILABILITY` still `Active` does it mean the scheduler still working? let's try to scale up.
```
docker@manager1:~$ docker service scale my_nginx=8                                                                                                                                          
my_nginx scaled to 8
overall progress: 8 out of 8 tasks 
1/8: running   [==================================================>] 
2/8: running   [==================================================>] 
3/8: running   [==================================================>] 
4/8: running   [==================================================>] 
5/8: running   [==================================================>] 
6/8: running   [==================================================>] 
7/8: running   [==================================================>] 
8/8: running   [==================================================>] 
verify: Service converged 

docker@manager1:~$ docker service ps my_nginx | grep Running                                                                                                                                
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 13 minutes ago                                  
hxbqfiqydow3        my_nginx.2          nginx:1.19          manager1            Running             Running 12 minutes ago                                  
9d0umdwmrkvu        my_nginx.3          nginx:1.19          manager1            Running             Running 9 minutes ago                                   
mvzbbce3305a        my_nginx.4          nginx:1.19          worker1             Running             Running 9 minutes ago                                   
sb3bxf2pzx7f        my_nginx.5          nginx:1.19          worker1             Running             Running 13 minutes ago                                  
6u8a459bflc3        my_nginx.6          nginx:1.19          manager1            Running             Running 12 minutes ago                                  
cs6l80aew15y        my_nginx.7          nginx:1.19          manager1            Running             Running 19 seconds ago                                  
u37hx2sd56hk        my_nginx.8          nginx:1.19          worker1             Running             Running 19 seconds ago
```
As you can see, even `AVAILABILITY: Active`, but `STATUS: Down` the scheduler can't assign it to that node.


- Candidate Manager with promoting `worker1` to `manager` as `Reachable` because currently there's still existing `manager`.
```
docker@manager1:~$ docker node promote worker1                                                                                                                                              
Node worker1 promoted to a manager in the swarm.

docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Active              Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active              Reachable           19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Down                Active                                  19.03.12
```
```
docker@worker1:~$ docker node ls                                                                                                                                                            
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6     manager1            Ready               Active              Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516 *   worker1             Ready               Active              Reachable           19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Down                Active                                  19.03.12
```
Let's try to `Drain` the `manager1`
```
docker@manager1:~$ docker node update --availability drain manager1                                                                                                                         
manager1

docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Drain               Leader              19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active              Reachable           19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Down                Active                                  19.03.12

docker@manager1:~$ docker service ps my_nginx | grep Running                                                                                                                                
djaxke61rm51        my_nginx.1          nginx:1.19          worker1             Running             Running 21 minutes ago                                      
eshbng7klvad        my_nginx.2          nginx:1.19          worker1             Running             Running about a minute ago                                  
muitdhb7wnvv        my_nginx.3          nginx:1.19          worker1             Running             Running about a minute ago                                  
mvzbbce3305a        my_nginx.4          nginx:1.19          worker1             Running             Running 17 minutes ago                                      
sb3bxf2pzx7f        my_nginx.5          nginx:1.19          worker1             Running             Running 21 minutes ago                                      
mybn1emoe04u        my_nginx.6          nginx:1.19          worker1             Running             Running about a minute ago                                  
irfw0pchkc9f        my_nginx.7          nginx:1.19          worker1             Running             Running about a minute ago                                  
u37hx2sd56hk        my_nginx.8          nginx:1.19          worker1             Running             Running 8 minutes ago
```
Right now, all services are running in `worker1`, but worker1 status still `Reachable` instead of `Leader`.\
Let's try to shutdown `manager1` machine.
```
root@verlandz:~# docker-machine stop manager1
Stopping "manager1"...
Machine "manager1" was stopped.
```
```
docker@worker1:~$ docker node ls                                                                                                                                                            
Error response from daemon: rpc error: code = Unknown desc = The swarm does not have a leader. It's possible that too few managers are online. Make sure more than half of the managers are online.
```
From that, we know that leader still doesn't change even `manager1` already `AVAILABILITY: Drain` and `STATUS: Down`.\
Let's up the all machines back, so we can proceed to clean up.
```
root@verlandz:~# docker-machine start manager1 worker2
```
```
docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Drain               Reachable           19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active              Leader              19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Ready               Active                                  19.03.12
```
After, the machines is back the `MANAGER STATUS` switched. Base on this [docs](https://docs.docker.com/engine/swarm/admin_guide/#add-manager-nodes-for-fault-tolerance) there's `fault tolerance`, that's might the answer why the `worker1` doesn't changed to `manager`\
Since all machines is active, let's promote the rest which `worker2` to `manager` + set back `AVAILABILITY` to `active` for `manager1`
```
docker@manager1:~$ docker node update --availability active manager1                                                                                                                        
manager1

docker@manager1:~$ docker node promote worker2                                                                                                                                              
Node worker2 promoted to a manager in the swarm.

docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Ready               Active              Reachable           19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Ready               Active              Leader              19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Ready               Active              Reachable           19.03.12
```
Okay now let's stop `worker1` machines, and let's see if `Leader` status is switched.
```
root@verlandz:~# docker-machine stop worker1
Stopping "worker1"...
Machine "worker1" was stopped.
```
```
docker@manager1:~$ docker node ls                                                                                                                                                           
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
ill4862sqfashbjr2njq967f6 *   manager1            Unknown             Active              Reachable           19.03.12
a8r8mzxsxssrf2szkqkspo516     worker1             Unknown             Active              Unreachable         19.03.12
zs6cilmrnps9f2wz2tx37g3rc     worker2             Unknown             Active              Leader              19.03.12
```
And yeah, we can still access with `docker node` in `manager1` + the leader already switched.


# 5.7. Clean Up
- leave swarm `docker swarm leave`, add `-f` to force it.
- remove docker services `docker service rm <SERVICE_NAME>`
- remove docker node `docker node rm <NODE_NAME>`, add `-f` to force it.
- remove docker-machines `docker-machine rm <MACHINE_NAME>`

***FAQ***\
**Q:** Is it good to have multiple services running on one node?\
**A:** No, it will caused performance issue. It's better to have one node, and one services.

**Q:** Is it possible to have more than 1 docker manager?\
**A:** Yes, but only one (primary) who do the work, while the others act as backup.